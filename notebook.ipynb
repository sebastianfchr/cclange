{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating virtual environment: .seb_huk_venv/bin/activate\n",
      "Requirement already satisfied: requests in ./.seb_huk_venv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.seb_huk_venv/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.seb_huk_venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.seb_huk_venv/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.seb_huk_venv/lib/python3.12/site-packages (from requests) (2025.1.31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['.seb_huk_venv/bin/pip', 'install', 'requests'], returncode=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# Define the virtual environment name\n",
    "venv_name = \".seb_huk_venv\"\n",
    "\n",
    "# Create the virtual environment when ran for the first time\n",
    "if not os.path.exists(venv_name):\n",
    "    print(f\"Creating virtual environment: {venv_name}\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"venv\", venv_name], check=True)\n",
    "\n",
    "# Activate virtual environment. Windows or Linux/MacOs\n",
    "if sys.platform == \"win32\": activate_script = os.path.join(venv_name, \"Scripts\", \"activate\")\n",
    "else: activate_script = os.path.join(venv_name, \"bin\", \"activate\")\n",
    "\n",
    "print(f\"Activating virtual environment: {activate_script}\")\n",
    "\n",
    "# Install dependencies, so the following code can run!\n",
    "pip_executable = os.path.join(venv_name, \"bin\", \"pip\") if sys.platform != \"win32\" else os.path.join(venv_name, \"Scripts\", \"pip\")\n",
    "subprocess.run([pip_executable, \"install\", \"requests\"], check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Now we can tart our docker for the \"service\" </u> \n",
    "To be honest, it's no real service. In reality I'd do this \n",
    "* in Kubernetes \n",
    "* or at least in docker-compose\n",
    "\n",
    "But to keep compatibility issues minimal, I'll strictly follow the exercise instructions, and just do a docker port-mapping to the host. (port 8123->8123).\n",
    "This will allow us to send requests to localhost:8123, and reach the server-api that's running on the docker. \n",
    "(Again, docker isn't the best way to do this. But we're talking fictional here)\n",
    "\n",
    "If notebook doesn't allow execution, you can always paste the commands below into your shell \n",
    "\n",
    "#### NOTE: It's much more reliable to paste these commands below into your shell directly. Don't trust Jupyter's execution (rights!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: driver failed programming external connectivity on endpoint awesome_jepsen (37d66b61813dd5de883e11466e9c4665abe221afb50954974a88954e43c818d8): Bind for 0.0.0.0:8123 failed: port is already allocated.\n"
     ]
    }
   ],
   "source": [
    "!docker run --gpus=all -v $(pwd):/code -p 8123:8123 -w /code -it sebastianfchr/appl_tfdocker:latest -- uvicorn serverapi:app --host 0.0.0.0 --port 8123\n",
    "\n",
    "# or if you use nerdctl like me, activate:\n",
    "# !nerdctl run --gpus=all -v $(pwd):/code -p 8123:8123 -w /code -it sebastianfchr/appl_tfdocker:latest -- uvicorn serverapi:app --host 0.0.0.0 --port 8123\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Docker image\n",
    "Above, we use my custom cuda-tf-docker, that I use for such deployments, hosted on docker container-registry `docker.io/sebastianfchr/appl_tfdocker:latest`\n",
    "\n",
    "It takes some time to download, since it's *cuda enabled*. (It runs tf2.7, and the compatible cuda/cudnn version)\n",
    "\n",
    "Generally, I'm a fan of lightweigtht docker images. But in this application, where we leverage the full potential of GPU-tensorflow, we have to go with this one.\n",
    "\n",
    "\n",
    "\n",
    "### Sending requests\n",
    "\n",
    "All we need here, is the python-package \"requests\". I've created two endpoints on the server for\n",
    "* single sentence prediction\n",
    "* prediction of \"chunks\" of sentences \n",
    "\n",
    "### API\n",
    "I made two api-endpoints:\n",
    "* `server-url/predict_sentence_batch/` for the chunked version\n",
    "* `server-url/predict_sentence/` for the single version\n",
    "\n",
    "\n",
    "Since all the functionality is on the docker, we merely need to be able to send requests from pyton. Let's predict some sentiments then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences and extractissons: \n",
      " \n",
      "`going down the beautiful road, I met a horrible rabbit` extract: `positive`: \n",
      " ==>  beautiful\n",
      "\n",
      "`Sadly, the guys from HUK gave me the wrong weights, and I had to do specification training myself` extract: `negative`: \n",
      " ==>  sadly,\n",
      "\n",
      "`I really hope that despite the whole python compatibility hell you could happily execute everything until here` extract: `negative`: \n",
      " ==>  hell\n",
      "\n",
      "`I really hope that despite the whole python compatibility hell you could happily execute everything until here` extract: `positive`: \n",
      " ==>  hope\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# docker is reachable through this mapping\n",
    "url_batch = \"http://0.0.0.0:8123/predict_sentence_batch/\"\n",
    "\n",
    "sentences = [\n",
    "    \"going down the beautiful road, I met a horrible rabbit\",\n",
    "    \"Sadly, the guys from HUK gave me the wrong weights, and I had to do specification training myself\",\n",
    "    \"I really hope that despite the whole python compatibility hell you could happily execute everything until here\",\n",
    "    \"I really hope that despite the whole python compatibility hell you could happily execute everything until here\"\n",
    "]\n",
    "sentiments = [\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"negative\",\n",
    "    \"positive\"\n",
    "]\n",
    "\n",
    "# POST-data for the batched sentence\n",
    "data = {\"sentences\": [s for s in sentences], \"sentiments\": [s for s in sentiments]}\n",
    "response = requests.post(url_batch, json=data, headers={ 'Content-Type': 'application/json' })\n",
    "\n",
    "try:\n",
    "    extracted_sentence_fragments = response.json()['data']\n",
    "    print(\"sentences and extractions: \\n \")\n",
    "    for orig_sentence, sentiment, extr_fragment in zip (sentences, sentiments, extracted_sentence_fragments):\n",
    "        print(\"`{}` extract: `{}`: \\n ==> {}\\n\".format(orig_sentence, sentiment, extr_fragment))\n",
    "\n",
    "except: \n",
    "    print(\"response seems not to contian json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> What about speed?</u>\n",
    "\n",
    "Very good question: This is where it gets interesting. There are some relevant comparisons to make, since we're dealing with a server that takes requests, but we want efficient execution. First, some facts\n",
    "\n",
    "* GPUS are latency-hiding machines that want \"compute-bound\", parallelizable programs. ðŸ‘Œ\n",
    "\n",
    "* GPUS have a n overhead for kernel-launches. So, launch the kernel on as many predictions (or as our GPU can)\n",
    "\n",
    "* Server requests have a round-trip time. That adds overhead to every request we make from the client\n",
    "\n",
    "* The above point means that we profit from chunking predictions within requests. But: Longer requests take a bit longer to send\n",
    "\n",
    "\n",
    "### A word about timing\n",
    "When we want to predict a number of sentences, \n",
    "we can investigate the time for the following modes.\n",
    "\n",
    "We differentiate between predictions that go through the API and those that don't:\n",
    "\n",
    "* single prediction request per API\n",
    "\n",
    "* chunked prediction requests per API\n",
    "\n",
    "* single direct prediction on tensorflow\n",
    "\n",
    "* single direct prediction on tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![title](\"./benchmarks.png\") -->\n",
    "\n",
    "<center><img src=\"benchmarks.png\" width=80% height=80% /></center>\n",
    "\n",
    "### <u> Why this is relevant </u>\n",
    "\n",
    "First, these numbers aren't surprising. But they tell the MLOps engineer something very interesting:\n",
    "* requests are best chunked before being called by tensorflor (and cuda). I'd love to go into depth here\n",
    "* requests are best chunked before being sent to the API to avoid unnecessary round-trip times\n",
    "* to the above point, we see a small influence of message-size even if we make chunked requests (orange-right) \n",
    "\n",
    "###  What the developers can do\n",
    "\n",
    "In a real a real setting, there will be small requests per  client.\n",
    "\n",
    "However, the engineer can make some design-improvements in case our servers are under high load:\n",
    "* aggregate incoming API-requests from different clients (over a small timeframe)\n",
    "* predict them in a chunked fashion (fast!)\n",
    "* map each prediction back them to the client, and send them back\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Generate your local benchmarks\n",
    "You can also generate your own! (you need to have the requirements.txt installed: just try to run the first code-section)\n",
    "This will generate a `benchmark_tf_and_api_calls.png` in this folder.\n",
    "\n",
    "#### Note: since the Challenge uses `tensorflow 2.7`, we must have a python-version between 2.6 and 2.9. Never ones don't run this tf! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([pip_executable, \"install\", \"--upgrade\", \"pip\"], check=True)\n",
    "subprocess.run([pip_executable, \"install\", \"-r\", \"requirements.txt\"], check=True)\n",
    "subprocess.run([\"python3\", \"--version\"], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/seb/Desktop/CodingChallenge_MLE/benchmark.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "ModuleNotFoundError: No module named 'pandas'\n"
     ]
    }
   ],
   "source": [
    "!python3 benchmark.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: pytest: command not found\n"
     ]
    }
   ],
   "source": [
    "# To run all my tests\n",
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two main things that I identified as testworthy\n",
    "* correct tf-model interfacing in deployment\n",
    "* correct API server behavior\n",
    "\n",
    "#### <u>1. Check for correct tf-model-interfacing and text-prediction: <br></u> `test_batched_sentence_extraction_vs_manual`\n",
    "The text goes through quite some processing before the data gets into the kernel-call. \n",
    "* Text preparation \n",
    "* Text tokenization \n",
    "* Masking\n",
    "* Prediction, tokenized output\n",
    "* Decoding of predicted tokens. This gives us the sentence-fragment\n",
    "\n",
    "Fully automating into the above used functions to\n",
    "\n",
    "`predict_sentence_batch(<<sentence>>, <<sentiment>>)` \n",
    "\n",
    "`predict_sentence(<<sentence>>, <<sentiment>>)` \n",
    "\n",
    "means to go through all the above stages. To test this code, I compared\n",
    "* a completely manually written version \n",
    "* the automatized versions above\n",
    "\n",
    "\n",
    "#### <u> 2. Check for correct API Behavior: </u> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
