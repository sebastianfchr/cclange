{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> ## HUK Coding Challenge </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br> <center> HUK Coding Challenge </center>\n",
    "#### <center> (Sebastian Christodoulou) </center>\n",
    "\n",
    "<center> Auf Englisch geschrieben wegen Terminologie, und weil meiner Erfahrung nach viele Developer das gewohnt sind </center>\n",
    "\n",
    "<h3>\n",
    "\n",
    "<u> Contents </u>\n",
    "* Launching the server with running LLM and API\n",
    "* Communication: Sending and receiving responses\n",
    "* Latency, measurements, potential optimizations\n",
    "* Testing-scenarios with pytest\n",
    "* A Dive into the software-architechture: Under the hood, and on the surface\n",
    "\n",
    "\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/seb/temp39venv/lib/python3.9/site-packages (2.32.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/seb/temp39venv/lib/python3.9/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/seb/temp39venv/lib/python3.9/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/seb/temp39venv/lib/python3.9/site-packages (from requests) (3.4.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !!!!!! ATTENTION !!!!!!!!!\n",
    "# Please use a venv for installations - else python blocks them\n",
    "%pip install -U requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Let's start the docker for the \"service\" </u> \n",
    "To be honest, we can barely call this a service. In reality I'd do this \n",
    "* in Kubernetes \n",
    "* or at least in docker-compose\n",
    "\n",
    "But to keep compatibility issues minimal, I'll strictly follow the exercise instructions, and just do a docker port-mapping to the host. (port 8123->8123).\n",
    "This will allow us to send requests to localhost:8123, and reach the server-api that's running on the docker. \n",
    "(Again, I'd do this in Kubetnetes)\n",
    "\n",
    "<u> NOTE </u>: It's may be more reliable to paste these commands below into your shell directly. Don't trust Jupyter's execution of docker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: Pulling from sebastianfchr/appl_tfdocker\n",
      "Digest: sha256:d91e8857a1cc7a240a0e38f8362911f19b4f53d314303939b7510299d69ac366\n",
      "Status: Image is up to date for sebastianfchr/appl_tfdocker:latest\n",
      "docker.io/sebastianfchr/appl_tfdocker:latest\n",
      "starting the docker. Please wait a bit until the API is running\n",
      "cb60188a801519c396cebfe2ef75fa04f3c36f1694f9d098884dd1131564f547\n"
     ]
    }
   ],
   "source": [
    "# Pull my deployment docker, specifically made for this task. It has the required tensorflow==2.7.0, and python3.9 [highest possible for tf2.7]\n",
    "!docker pull sebastianfchr/appl_tfdocker:latest\n",
    "print(\"starting the docker. Please wait a bit until the API is running\")\n",
    "\n",
    "!docker run -d --gpus=all -v $(pwd):/code -p 8123:8123 -w /code sebastianfchr/appl_tfdocker:latest -- uvicorn serverapi:app --host 0.0.0.0 --port 8123\n",
    "\n",
    "# or if you use nerdctl like me:\n",
    "# nerdctl pull sebastianfchr/appl_tfdocker:latest\n",
    "# nerdctl run -d --gpus=all -v $(pwd):/code -p 8123:8123 -w /code sebastianfchr/appl_tfdocker:latest -- uvicorn serverapi:app --host 0.0.0.0 --port 8123\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <u>Docker image</u>\n",
    "Above, we use my custom cuda-tf-docker, that I use for such deployments, hosted on docker container-registry `docker.io/sebastianfchr/appl_tfdocker:latest`\n",
    "\n",
    "It takes some time to download, since it's *cuda enabled*. (It runs tf2.7, and the compatible cuda/cudnn version)\n",
    "\n",
    "Generally, I'm a fan of lightweigtht docker images. But in this application, where we leverage the full potential of GPU-tensorflow, we have to go with this one.\n",
    "\n",
    "\n",
    "### Sending requests\n",
    "\n",
    "All we need here, is the python-package \"requests\". I've an API endpoint on the server that can be used for\n",
    "* single sentence prediction\n",
    "* prediction of \"chunks\" of sentences \n",
    "\n",
    "### API\n",
    "I has one endpoint:\n",
    "* `server-url/predict_sentence_batch/` for the chunked version\n",
    "\n",
    "\n",
    "Since all the functionality is in the docker, we merely need to be able to send requests from pyton. Let's predict some sentiments then\n",
    "\n",
    "\n",
    "### <center> Important: Please make sure the docker is up and running. Its API-server takes a bit to load </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ========================= Sentences, sentiments and predicted fragments ================================\n",
      "\n",
      "`going down the beautiful road, I met a horrible rabbit` \n",
      " ==(positive sentiment)==> ` beautiful`\n",
      "\n",
      "\n",
      "`going down the beautiful road, I met a horrible rabbit` \n",
      " ==(negative sentiment)==> ` horrible rabbit`\n",
      "\n",
      "\n",
      "`Sadly, the guys from HUK gave me the wrong weights, and I had to do specification training myself` \n",
      " ==(negative sentiment)==> ` sadly,`\n",
      "\n",
      "\n",
      "`I really hope that despite the whole python compatibility hell you could happily execute everything until here` \n",
      " ==(negative sentiment)==> ` hell`\n",
      "\n",
      "\n",
      "`I really hope that despite the whole python compatibility hell you could happily execute everything until here` \n",
      " ==(positive sentiment)==> ` hope`\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# docker is reachable through this mapping\n",
    "url_batch = \"http://0.0.0.0:8123/predict_sentence_batch/\"\n",
    "\n",
    "sentences = [\n",
    "    \"going down the beautiful road, I met a horrible rabbit\",\n",
    "    \"going down the beautiful road, I met a horrible rabbit\",\n",
    "    \"Sadly, the guys from HUK gave me the wrong weights, and I had to do specification training myself\",\n",
    "    \"I really hope that despite the whole python compatibility hell you could happily execute everything until here\",\n",
    "    \"I really hope that despite the whole python compatibility hell you could happily execute everything until here\"\n",
    "]\n",
    "sentiments = [\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"negative\",\n",
    "    \"negative\",\n",
    "    \"positive\"\n",
    "]\n",
    "\n",
    "# POST-data for the batched sentence\n",
    "data = {\"sentences\": [s for s in sentences], \"sentiments\": [s for s in sentiments]}\n",
    "response = requests.post(url_batch, json=data, headers={ 'Content-Type': 'application/json' })\n",
    "\n",
    "try:\n",
    "    extracted_sentence_fragments = response.json()['data']\n",
    "\n",
    "    print(\"\\n\\n ========================= Sentences, sentiments and predicted fragments ================================\\n\")\n",
    "    print(\"\\n\".join( [\"`{}` \\n ==({} sentiment)==> `{}`\\n\\n\".format(t,s,p) for t,s,p in zip(sentences, sentiments, extracted_sentence_fragments)]))\n",
    "\n",
    "\n",
    "except: \n",
    "    print(\"response seems not to contian json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> What about latency?</u>\n",
    "\n",
    "Very good question: This is where it gets interesting. There are some relevant comparisons to make, since we're dealing with a server that takes requests, but we want efficient execution. First, some facts\n",
    "\n",
    "* GPUS are latency-hiding machines that perform well on \"compute-bound\", parallelizable programs. ML generally fits this.\n",
    "\n",
    "* GPUS have a n overhead for kernel-launches. So, launch the kernel on as many predictions (or as our GPU can)\n",
    "\n",
    "* Server requests have a round-trip time. That adds overhead to every request we make from the client\n",
    "\n",
    "* The above point means that we profit from chunking predictions within requests. But: Longer requests take a bit longer to send\n",
    "\n",
    "\n",
    "### A word about speed\n",
    "When we want to predict a number of sentences, \n",
    "we can investigate the time for the following modes.\n",
    "\n",
    "We differentiate between predictions that go through the API and those that don't:\n",
    "\n",
    "* single prediction request per API (right, blue)\n",
    "\n",
    "* chunked prediction requests per API (right, orange)\n",
    "\n",
    "* single direct prediction on tensorflow (left, blue)\n",
    "\n",
    "* single chinked prediction on tensorflow (left, orange)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![title](\"./benchmarks.png\") -->\n",
    "\n",
    "<center><img src=\"./images/benchmarks.png\" width=70% height=70% /></center>\n",
    "\n",
    "### <u> Why this is relevant </u>\n",
    "\n",
    "These measurements aren't surprising. But they tell the MLOps engineer something very relevant:\n",
    "* requests are best chunked before being called by tensorflor (and cuda. We can go into depth here)\n",
    "* requests are best chunked before being sent to the API to avoid unnecessary round-trip times\n",
    "* to the above point, we see a small influence of message-size on the round-trip-time of large chunked requests (orange-right) \n",
    "\n",
    "###  What the developers can do\n",
    "\n",
    "In a real a real setting, there will be small requestss per client, and a longer round-trip time.\n",
    "\n",
    "Thus, the engineer can make some design-improvements in case the servers are under very high load:\n",
    "* aggregate incoming API-requests from different clients once they reach the server (over a small acceptable timeframe)\n",
    "* predict them in a chunked fashion (fast!)\n",
    "* map each prediction back them to the client, and send them back\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Note: To run the rest, wee need to install requirements_strict.txt:<br> since the Challenge uses `tensorflow 2.7`, we must have a compatible python-version between 2.6 and 2.9. We can use another tf-version of a newer python, but this might run into confilict with transformers! \n",
    "\n",
    "requirements_strict.txt represent the requirements demanded in the challenge (i.e. tf2.7 => python3.6-3.9). If you encounter the exception below, try commenting it out and thus installing requirements_relaxed.txt (no guarantees!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/seb/temp39venv/lib/python3.9/site-packages (from -r requirements/requirements_strict.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: tensorflow==2.7 in /home/seb/temp39venv/lib/python3.9/site-packages (from -r requirements/requirements_strict.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: numpy==1.* in /home/seb/temp39venv/lib/python3.9/site-packages (from -r requirements/requirements_strict.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/seb/temp39venv/lib/python3.9/site-packages (from -r requirements/requirements_strict.txt (line 4)) (1.6.1)\n",
      "Requirement already satisfied: transformers==4.27.4 in /home/seb/temp39venv/lib/python3.9/site-packages (from -r requirements/requirements_strict.txt (line 5)) (4.27.4)\n",
      "Requirement already satisfied: protobuf==3.20.* in /home/seb/temp39venv/lib/python3.9/site-packages (from -r requirements/requirements_strict.txt (line 6)) (3.20.3)\n",
      "Requirement already satisfied: fastapi in /home/seb/temp39venv/lib/python3.9/site-packages (from -r requirements/requirements_strict.txt (line 7)) (0.115.12)\n",
      "Requirement already satisfied: pytest in /home/seb/temp39venv/lib/python3.9/site-packages (from -r requirements/requirements_strict.txt (line 8)) (8.3.5)\n",
      "Requirement already satisfied: matplotlib in /home/seb/temp39venv/lib/python3.9/site-packages (from -r requirements/requirements_strict.txt (line 9)) (3.9.4)\n",
      "Requirement already satisfied: httpx in /home/seb/temp39venv/lib/python3.9/site-packages (from -r requirements/requirements_strict.txt (line 10)) (0.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (4.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (0.45.1)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (18.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (2.19.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (1.71.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (3.13.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (1.17.2)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: filelock in /home/seb/temp39venv/lib/python3.9/site-packages (from transformers==4.27.4->-r requirements/requirements_strict.txt (line 5)) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from transformers==4.27.4->-r requirements/requirements_strict.txt (line 5)) (0.29.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from transformers==4.27.4->-r requirements/requirements_strict.txt (line 5)) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/seb/temp39venv/lib/python3.9/site-packages (from transformers==4.27.4->-r requirements/requirements_strict.txt (line 5)) (2024.11.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from transformers==4.27.4->-r requirements/requirements_strict.txt (line 5)) (24.2)\n",
      "Requirement already satisfied: requests in /home/seb/temp39venv/lib/python3.9/site-packages (from transformers==4.27.4->-r requirements/requirements_strict.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from transformers==4.27.4->-r requirements/requirements_strict.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/seb/temp39venv/lib/python3.9/site-packages (from transformers==4.27.4->-r requirements/requirements_strict.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/seb/temp39venv/lib/python3.9/site-packages (from pandas->-r requirements/requirements_strict.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from pandas->-r requirements/requirements_strict.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/seb/temp39venv/lib/python3.9/site-packages (from pandas->-r requirements/requirements_strict.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from scikit-learn->-r requirements/requirements_strict.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from scikit-learn->-r requirements/requirements_strict.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from scikit-learn->-r requirements/requirements_strict.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from fastapi->-r requirements/requirements_strict.txt (line 7)) (0.46.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /home/seb/temp39venv/lib/python3.9/site-packages (from fastapi->-r requirements/requirements_strict.txt (line 7)) (2.11.1)\n",
      "Requirement already satisfied: tomli>=1 in /home/seb/temp39venv/lib/python3.9/site-packages (from pytest->-r requirements/requirements_strict.txt (line 8)) (2.2.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/seb/temp39venv/lib/python3.9/site-packages (from pytest->-r requirements/requirements_strict.txt (line 8)) (1.2.2)\n",
      "Requirement already satisfied: iniconfig in /home/seb/temp39venv/lib/python3.9/site-packages (from pytest->-r requirements/requirements_strict.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/seb/temp39venv/lib/python3.9/site-packages (from pytest->-r requirements/requirements_strict.txt (line 8)) (1.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from matplotlib->-r requirements/requirements_strict.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/seb/temp39venv/lib/python3.9/site-packages (from matplotlib->-r requirements/requirements_strict.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from matplotlib->-r requirements/requirements_strict.txt (line 9)) (6.5.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from matplotlib->-r requirements/requirements_strict.txt (line 9)) (4.56.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/seb/temp39venv/lib/python3.9/site-packages (from matplotlib->-r requirements/requirements_strict.txt (line 9)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from matplotlib->-r requirements/requirements_strict.txt (line 9)) (3.2.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from matplotlib->-r requirements/requirements_strict.txt (line 9)) (1.4.7)\n",
      "Requirement already satisfied: anyio in /home/seb/temp39venv/lib/python3.9/site-packages (from httpx->-r requirements/requirements_strict.txt (line 10)) (4.9.0)\n",
      "Requirement already satisfied: idna in /home/seb/temp39venv/lib/python3.9/site-packages (from httpx->-r requirements/requirements_strict.txt (line 10)) (3.10)\n",
      "Requirement already satisfied: certifi in /home/seb/temp39venv/lib/python3.9/site-packages (from httpx->-r requirements/requirements_strict.txt (line 10)) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/seb/temp39venv/lib/python3.9/site-packages (from httpx->-r requirements/requirements_strict.txt (line 10)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/seb/temp39venv/lib/python3.9/site-packages (from httpcore==1.*->httpx->-r requirements/requirements_strict.txt (line 10)) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.4->-r requirements/requirements_strict.txt (line 5)) (2025.3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements/requirements_strict.txt (line 9)) (3.21.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r requirements/requirements_strict.txt (line 7)) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r requirements/requirements_strict.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r requirements/requirements_strict.txt (line 7)) (2.33.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from anyio->httpx->-r requirements/requirements_strict.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (3.1.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (58.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/seb/temp39venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/seb/temp39venv/lib/python3.9/site-packages (from requests->transformers==4.27.4->-r requirements/requirements_strict.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from requests->transformers==4.27.4->-r requirements/requirements_strict.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/seb/temp39venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (8.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/seb/temp39venv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.7->-r requirements/requirements_strict.txt (line 2)) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "(py_major, py_minor) = sys.version_info[0:2]\n",
    "\n",
    "if not (py_major == 3 and (6 <= py_minor <= 9)):\n",
    "    # NOTE: You can comment this out at your own peril.\n",
    "    raise Exception('Requires python 3.6-3.9, else you cannot run the code provided by HUK. '\n",
    "    'You can comment out this exception, and see whether it works with a newer Setup. \\n \\\n",
    "     The specification Tensorflow==2.7.0 is only compatible with 3.6-3.9. Similar things hold for transformers ')\n",
    "\n",
    "    %pip install -r requirements/requirements_relaxed.txt\n",
    "\n",
    "else: \n",
    "    %pip install -r requirements/requirements_strict.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate your own benchmarks locally\n",
    "You can now generate your own benchmarks, with `benchmarks.py`. The code below will generate a `benchmark_tf_and_api_calls.png` in this folder.\n",
    "\n",
    "\n",
    "<u>GPU Warning</u>: Since the docker is running tf-gpu instance AND the benchmarking-script runs another one, this line will fail if your machine has less than ~4GB GPU vRam (~ 2GB reserved in docker, ~2 GB reserved here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Python tests: Relevant Test Concepts </u>\n",
    "1. The interfacing with the tf-model is done correctly\n",
    "2. The API server behaves correctly\n",
    "\n",
    "These things are realized in `test_components.py`\n",
    "\n",
    "\n",
    "##### <u> Test 1. Check for correct tf-model-interfacing and text-prediction:</u>\n",
    "The text goes through quite some processing before the data gets into the kernel-call. \n",
    "* Text preparation \n",
    "* Text tokenization \n",
    "* Masking\n",
    "* Prediction, tokenized output\n",
    "* Decoding of predicted tokens. This gives us the sentence-fragment\n",
    "\n",
    "I fully automated this process in the below functions \n",
    "* for batches of sentences `predict_sentence_batch(<<sentence_list>>, <<sentiment_list>>)` \n",
    "* or for single sentences `predict_sentence(<<sentence>>, <<sentiment>>)` \n",
    "\n",
    "So that I can predict on text directly. To test this code, I compared\n",
    "* a completely manually written version of the tokenization-prediction-decoding pipeline \n",
    "* my automatized versions \n",
    "\n",
    "\n",
    "##### <u> Tests 2. Check for correct API Behavior: </u> \n",
    "We want an API that reacts to semantic and structural errors. Following things can go wrong \n",
    "* Incorrectly structured POST-Request (Message with wrong fields. We expect it to have 'sentences' and 'sentiments')\n",
    "* Sentiments are all either [\"positive\", \"negative\", \"neutral\"]\n",
    "* Sentences or sentiments cannot be empty\n",
    "* For Batched prediction: One sentiments per sentence  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<u>GPU WARNING</u>: Since the docker is running tf-gpu instance AND the testing runs another tf-gpu instance, this line will fail if your machine has less than ~4GB GPU vRam (~ 2GB occupied per instance). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.21, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: /home/seb/Desktop/CodingChallenge_MLE\n",
      "plugins: anyio-4.9.0\n",
      "collected 5 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_components.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../temp39venv/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:209\n",
      "  /home/seb/temp39venv/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:209: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "    np.bool8: (False, True),\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m========================= \u001b[32m5 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 9.64s\u001b[0m\u001b[33m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# To run all my tests, do this\n",
    "# it tests all files which start with `test_`. In our case, just `test_components.py`\n",
    "\n",
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Train your own weights </u> \n",
    "\n",
    "After refractoring a lot, I only left the training-loop itself intact. Check out the following program to train your own weights. \n",
    "\n",
    "\n",
    "<u>Note</u>: I found that this line does not run unless you comply with requirements_strict. <br>It looks like huggingface's transformers relied on tf-calls that changed in later versions (that is, later than about tf2.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <u> Software-Design and Refractoring </u>\n",
    "\n",
    "I turned this into the 'roberta' package. The classes \n",
    "* `TokenEncoder` and\n",
    "* `RobertaPredictor`\n",
    "allow me to be able to write more understandable code\n",
    "\n",
    "These work on two differnt levels: \n",
    "* TokenEncoder prepares input (from any set that follows our training-data conventions) into input_ids_t, attention_mask_t, token_type_ids_t\n",
    "* RobertaPredictor perform predictions \n",
    "    1. on tokens, or \n",
    "    2. direclty from text (converts text to tokens, predicts tokens, then decodes)\n",
    "\n",
    "\n",
    "Here, we use \n",
    "1. TokenEncoder to turn data into tokens, and apply the encode-predict-decode pipeline manually (of course, we can always go lower by looking into its member-functions).\n",
    "2. RobertaPredictor automatizes encoding and prediction. It just takes a sentence and sentiment as arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 00:36:51.283847: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2025-03-31 00:36:51.283863: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/seb/temp39venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-31 00:36:52.884618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-03-31 00:36:52.887319: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2025-03-31 00:36:52.887350: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2025-03-31 00:36:52.887372: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2025-03-31 00:36:52.887395: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2025-03-31 00:36:52.920760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2025-03-31 00:36:52.920808: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2025-03-31 00:36:52.920814: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-03-31 00:36:52.921194: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at /home/seb/Desktop/CodingChallenge_MLE/roberta/config/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ========================= Sentences, sentiments and predicted fragments ================================\n",
      "\n",
      "`Last session of the day  http://twitpic.com/67ezh` \n",
      " ==(neutral sentiment)==> ` last session of the day`\n",
      "\n",
      "\n",
      "` Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).` \n",
      " ==(positive sentiment)==> ` exciting`\n",
      "\n",
      "\n",
      "`Recession hit Veronique Branquinho, she has to quit her company, such a shame!` \n",
      " ==(negative sentiment)==> ` such a shame!`\n",
      "\n",
      "\n",
      "` happy bday!` \n",
      " ==(positive sentiment)==> ` happy bday!`\n",
      "\n",
      "\n",
      "` http://twitpic.com/4w75p - I like it!!` \n",
      " ==(positive sentiment)==> ` i like it!!`\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from roberta import RobertaPredictor, TokenEncoder \n",
    "import tokenizers\n",
    "import pandas as pd\n",
    "\n",
    "MAX_LEN = 96\n",
    "\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer.from_file('./roberta/config/vocab-roberta-base.json', './roberta/config/merges-roberta-base.txt', lowercase=True, add_prefix_space=True) \n",
    "rp = RobertaPredictor(MAX_LEN, 'v0-roberta-0.h5', tokenizer)\n",
    "\n",
    "# 1) RAW VERSION:  \n",
    "# Use inputs from tokenization, manual tokenized prediction, and decoding of prediction to sentence-fragments\n",
    "test = pd.read_csv('./data/test.csv').fillna('')\n",
    "e = TokenEncoder(MAX_LEN, tokenizer)\n",
    "input_ids_t, attention_mask_t, _ = e.prepare_encode_test(test)\n",
    "# ids and attention_masks. Predict left end right end of subsequence-range within ids \n",
    "ids, ams = input_ids_t[0:5], attention_mask_t[0:5]\n",
    "ls, rs= rp.predict_tokenized(ids, ams)\n",
    "# Based on (l, r), obtain the extracted ranges        \n",
    "predicted_subsequences = [ids[i, l:r+1] for i, (l,r) in enumerate(zip(ls,rs))]\n",
    "# decode them into sentence sub-fragments via tokenizer\n",
    "sentence_fragments_predicted_manual = list(map(tokenizer.decode, predicted_subsequences))\n",
    "\n",
    "print(\"\\n\\n ========================= Sentences, sentiments and predicted fragments ================================\\n\")\n",
    "print(\"\\n\".join( [\"`{}` \\n ==({} sentiment)==> `{}`\\n\\n\".format(t,s,p) for t,s,p in zip(test['text'],test['sentiment'], sentence_fragments_predicted_manual)]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above is just what's happening under the hood. As promised, my software does this via much simpler calls, since RobertaPredictor can predict from text directly (and performs the intermediate steps automatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ========================= Sentences, sentiments and predicted fragments ================================\n",
      "\n",
      "`Last session of the day  http://twitpic.com/67ezh` \n",
      " ==(neutral sentiment)==> ` last session of the day`\n",
      "\n",
      "\n",
      "` Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).` \n",
      " ==(positive sentiment)==> ` exciting`\n",
      "\n",
      "\n",
      "`Recession hit Veronique Branquinho, she has to quit her company, such a shame!` \n",
      " ==(negative sentiment)==> ` such a shame!`\n",
      "\n",
      "\n",
      "` happy bday!` \n",
      " ==(positive sentiment)==> ` happy bday!`\n",
      "\n",
      "\n",
      "` http://twitpic.com/4w75p - I like it!!` \n",
      " ==(positive sentiment)==> ` i like it!!`\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('./data/test.csv').fillna('')\n",
    "\n",
    "sentences_fragments_automatic = rp.predict_sentence_batch(test['text'][0:5], test['sentiment'][0:5])\n",
    "\n",
    "\n",
    "print(\"\\n\\n ========================= Sentences, sentiments and predicted fragments ================================\\n\")\n",
    "print(\"\\n\".join( [\"`{}` \\n ==({} sentiment)==> `{}`\\n\\n\".format(t,s,p) for t,s,p in zip(test['text'],test['sentiment'], sentences_fragments_automatic)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37venv)\n",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
